import tensorflow as tf
x_ = tf.placeholder(tf.float32, shape=[4,2], name="x-input")
y_ = tf.placeholder(tf.float32, shape=[4,1], name="y-input")
Theta1 = tf.Variable(tf.random_uniform([2,1], 0, 1), name="Theta1")
Bias1 = tf.Variable(tf.zeros([2]), name="Bias1")
Hypothesis= tf.sigmoid(tf.matmul(x_, Theta1) + Bias1)
cost = tf.reduce_mean(( (y_ * tf.log(Hypothesis)) + 
        ((1 - y_) * tf.log(1.0 - Hypothesis)) ) * -1)
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
AND_X = [[0,0],[0,1],[1,0],[1,1]]
AND_Y = [[0],[0],[0],[1]]

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
for i in range(10000):
        sess.run(train_step, feed_dict={x_: AND_X, y_: AND_Y})
        
        if i % 100 == 0:
            print('Epoch ', i)
            print('Hypothesis ', sess.run(Hypothesis, feed_dict={x_: AND_X, y_: AND_Y}))
            print('Theta1 ', sess.run(Theta1))
            print('Bias1 ', sess.run(Bias1))
            print('cost ', sess.run(cost, feed_dict={x_: AND_X, y_: AND_Y}))
            print('---------------------------------------------------------------------------------------------------')
